---
title: "text mining"
author: "Christina"
date: "11/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(gutenbergr)
library(wordcloud)
library(reshape2)
```

# Download and Tidy

```{r}
## Download the book from gutenberg
jane <- gutenberg_download(1260)
## Annotate a 'linenumber' quantity to keep track of lines in the original format and a 'chapter'(with a regex) to find where all the chapters are
original_books <-jane %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  unnest_tokens(word, text)

original_books
## Restructure into the one-token-per-row format with 'unnest_tokens()' function. Here the tokenizing is for words
tidy_books<-original_books %>%
  unnest_tokens(word, word)

length(unique(tidy_books$chapter))
count(tidy_books, chapter) # In this book, there are 39 chapters in it
## Manipulate with tidy tools

# Remove stop words -- remove stop words(kept in the tidytext dataset 'stop_words') with 'anti_join()'
data("stop_words")

tidy_books<-tidy_books %>%
  anti_join(stop_words) #'stop_words' dataset in the tidytext package contains stop words from three lexicons -- "AFINN", "bing","nrc"
  
# Find the most common words with dplyr's 'count()'
tidy_books %>%
  count(word, sort = TRUE) # The word counts are stored in a tidy data frame, allowing us to pipe this directly to the ggplot2 package

# Visualization of the most common words
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) # Here I filter the number of words > 200 in this book and we can see the most common word is jane.
```

# Sentiment analysis with tidy data

In the previous part, I explored what changed the original book into the tidy text format and showed how this format can be used to approach questions about word frequency. Next I would like to address the topic of sentiment analysis. When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative. So here I use the tools of text mining to appraoch the emotional content of text.

```{r}
## Sentiment analysis with inner join
# Look at the words with a joy score from the NRC lexicon
nrc_joy<-get_sentiments("nrc") %>%
  filter(sentiment == "joy") # Choose the name 'word' for the output column from 'unnest_tokens()' and filter for the joy words

# What are the most common joy words in Jane Eyre
tidy_books %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) 

```

I found that mostly positive, happy words about love, hope and happy here.

In addition, I want to examine how sentiment changes throughout each novel. First, I find a sentiment score for each word using the Bing lexicon and 'inner_join()'. Next I count up how many positive and negative words there are in defined sections of the book and define an "index" here to keep track of where we are in the narrative; this index (using integer division) counts up sections of 60 lines of text. In addition, I use 'pivot_wider()' so that we can have negative and positive sentiment in separate columns and finally calculate a net sentiment (positive - negative).

```{r}
# Calculate negative and positive sentiment
jane_sentiment<-tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 60, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

# Plot sentiment scores
ggplot(jane_sentiment,aes(index, sentiment,)) +
  geom_col(show.legend = FALSE) 
  # Notice that here I'm plotting against the 'index' on the x-axis that keeps track of narrative time in sections of text.
```

We can see how the plot of each novel changes toward more positive or negative sentiment over the trajectory of the story. From the plot above, I find that there are more negative sentiments during the early index and more positive sentiments during the late index. I think this fits the story. During Eyre's childhood, she was abuted by her aunt,Sarah Reed, she did not have a happy childhood in her early years. Therefore the sentiment is negative. But when she met Rochester as written in the late plots, she was married with him and had a sweet life. So there are more positive ones here.

# Compare the three sentiment dictionaries
 - AFINN: the AFINN lexicon measures sentiment with a numeric score between -5 and 5;
 - Bing: the Bing lexicon categorizes words in a binary fashion into positive and negative categories;
 - nrc: the nrc lexicon categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise and trust.

```{r}
# AFINN
afinn<-tidy_books %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber %/% 60) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")
# Bing
bing_and_nrc<-bind_rows(
  tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing at al."),
  tidy_books %>%
    inner_join(get_sentiments("nrc") %>%
                 filter(sentiment %in% c("positive","negative"))
               ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 60, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative)

# Visualize
bind_rows(afinn,
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

The three different lexicons for calculating sentiment give results that are different in an absolute sense but have similar relative trajectories through the novel. We can see similar dips and peaks in sentiment at about the same places in the novel, but the absolute values are significantly different. The AFINN lexicon gives the largest absolute values, with high positive values. The Bing lexicon has lower absolute values. The NRC lexicon are shifted higher realtive to the other two, labeling the text more positively.

```{r}
## Why the result for the NRC lexicon biased so high in sentiment compared to the Bing result?
# How many positive and negative words are in these lexicons?
get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  count(sentiment)

get_sentiments("bing") %>%
  count(sentiment)
```

Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon. This will contribute to the effect we see in the plot above, as will any systematic difference in word matches, e.g. if the negative words in the NRC lexicon do not match the words that Jane Austen uses very well. Whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon. This is all important context to keep in mind when choosing a sentiment lexicon for analysis.

# Most common positive and negative words -- One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment

```{r}
# How much each word contributed to each sentiment?
bing_word_counts<-tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

# Show visually
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
The plot above shows words that contribute to positive and negative sentiment in Eyre. 

# Wordclouds

```{r}
## Wordclouds
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

## Use 'comparison.cloud()' function -- turn the data frame into a matrix
# Here I did the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words.
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

The size of a word's text is in proportion to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words. Here the most important positive word is 'positive' and the most important negative word is 'miss'.

### Task three

Truenumbers (TN) is a system for building data resources that are hosted on a server and accessed through clients that interact with with server through the Truenumbers API. The system uses natural language descriptions of data items and includes a tagging function for augmenting data items, defining subsets, and tracking process metadata. Following I would like to use TN to do analysis on Eyre book.

## Download packages and the book

```{r}
## Install the R tnum package 
library(devtools)
devtools::install_github("Truenumbers/tnum/tnum")

#'tnum' package
library(knitr)
library(gutenbergr)
library(tidyverse)
library(tnum)

tnum.authorize("mssp1.bu.edu")
tnum.setSpace("test2")

#jane_eyre<-gutenberg_download(gutenberg_id = 1260)  ## download Jane Eyre
jane_eyre_txt<-readLines("jane.txt")
source("Book2TN-v3-hw.R")

#tnBooksFromLines(jane_eyre_txt, "je")


## not run: tnBooksFromLines(jane_eyre$text, "jane_eyre") -- it took me a very long time

tnum.getDBPathList(taxonomy = "subject", levels = 2) # make sure the book be ingested properly
```

Assign the return from 'tnum.query' function to a variable so that we can examine the list items in the environment. And then convert the TN list to a data frame (use the 'tnum.objectstoDF()' function) so that each TN is a row in the data frame.

```{r echo=TRUE, warning=FALSE, message=FALSE}
q111 <- tnum.query(query = "jane_eyre# has ordinal", max=500)   ## everything
df111 <- tnum.objectsToDf(q111)

## show ordered objects in document
q112 <- tnum.query("jane_eyre# has ordinal")   ## show ordered objects in document
df112 <- tnum.objectsToDf(q112)

## focus on one paragraph -- note the word count for each sentence
q3 <- tnum.query("jane_eyre/section:0001/paragraph:0005# has count#")  # just 1 para
df3 <- tnum.objectsToDf(q3) %>% filter(date == "2021-12-03")
df3


## and now look at the text in a sentence
q1 <- tnum.query("jane_eyre/section:0001/paragraph:0005/sentence:0002# has text")
df1 <- tnum.objectsToDf(q1) %>% filter(date == "2021-12-03")
df1

## To extract a paragraph of text
q4 <- tnum.query("jane_eyre/section:0001/paragraph:0005/sentence# has text", max = 10)
df4 <- tnum.objectsToDf(q4) %>% filter(date == "2021-12-03")
para_text4 <- df4 %>% pull(string.value) %>% 
                      str_replace_all("\"","") %>% 
                      str_flatten(collapse = " ")

## steps to understand
# a <- para_text4[1]
# a
# 
# b <- str_replace_all(a,"\"","")   
# b
# 
# c <- para_text4
# c
# 
# c <- str_replace_all(a,"\"","")  
# 
```

## Use tnum in text analysis

In this part, I want to use TNs for text analysis. As I did before, loading the libraries I need, authorizing the server if needed and setting the number space to "test2". 

```{r}
##  The ordinal numbers for the entire book 
##  show the sequence of objects in order of their appearance.
w10 <- tnum.query("jane_eyre# has ordinal", max=1800)
wdf10 <- tnum.objectsToDf(w10)

## Examing the first 50 TNs  makes it easy to see the Table of Contents
## and to see that object 11 is the heading at the start of Chapter 1


## This shows the Table of Contents
w11 <- tnum.query("jane_eyre# has text", start = 3 ,max=18)
wdf11 <- tnum.objectsToDf((w11))

table_of_contents <- wdf11 %>% select(string.value) 


## Look at just the headings shows the structure of the book
w13 <- tnum.query("jane_eyre/heading# has text", max=40)
wdf13 <- tnum.objectsToDf(w13)


## It may look like the table of contents is repeated twice,
## but examing the ordinals produces chapter list that includes the 
## ordinal location for the heading of each chapter
w14 <- tnum.query("jane_eyre/heading# has ordinal", max=40)
wdf14 <- tnum.objectsToDf(w14)

chapter_locations <- left_join(select(wdf13, subject, string.value), 
                               select(wdf14, subject, numeric.value)) 
## add column for chapter number
library(magrittr)
chapter_locations %<>% mutate(chapter=1:36) # because there are 36 chapters in total

w15 <- tnum.query("jane_eyre/section:0011# has ordinal")
wdf15 <- tnum.objectsToDf(w15)



## chapter 1 para 1, word counts for the 3 sentences in para 1
q20 <- tnum.query("jane_eyre# has *", max=3)
df20 <- tnum.objectsToDf(q20)

#  chapter locations  ordinal numbers
ord_ch1 <- unlist(tnum.query("jane_eyre/heading:0011# has ordinal"))
ord_ch2 <- unlist(tnum.query("jane_eyre/heading:0012# has ordinal"))


ch1_txt <- tnum.query("jane_eyre/section:0011/paragraph:0002/# has text", max=30)

ch1_txt_df <- tnum.objectsToDf(ch1_txt)
ch1_txt_df$string.value

ch2_txt <- tnum.query("jane_eyre/section:0011/paragraph:0002/sentence:# has *", max=30)
ch2_txt_df <- tnum.objectsToDf(ch2_txt)

ch2_txt_df$string.value

length(ch2_txt_df$string.value)


q21 <- tnum.query("jane_eyre/section:0011/paragraph:0001/# has *", max = 30)
df21 <- tnum.objectsToDf(q21)
```

## Sentimentr

```{r}
library(sentimentr)
jane_1 <- get_sentences(para_text4)

## to get sentiment scores by sentence
sentiment(jane_1)

## to get sentiment scores aggregated by paragraph
sentiment_by(jane_1)

emotion(jane_1)
profanity(jane_1)

jan<-tidy_books
jan %>%
  get_sentences() %>%
  sentiment() -> jan_senti

jan_senti %>%
  ggplot() + geom_density(aes(sentiment))

jan_senti %>%
  ggplot() + geom_boxplot(aes(x = linenumber, y = sentiment))

jan_senti %>%
  mutate(polarity_level = ifelse(sentiment > 0, "Positive", "Negative")) %>%
  count(linenumber, polarity_level) %>%
  ggplot() + geom_col(aes(y = linenumber, x = n, fill = polarity_level)) +
  theme_minimal()

jane_1 %>%
  get_sentences() %>%
  sentiment_by() %>%
  highlight()

```

#Sense and Sensibility

```{r}
#create tag
qr2<-tnum.query("jane_eyre/# has*=REGEXP(\" Jane|Reed\")",max = 50)
qr_df_2<-tnum.objectsToDf(qr2)
tnum.tagByQuery("jane_eyre/# has*=REGEXP(\" Jane|Reed\")",adds = ("reference:jr"))

#make plots(before making plots, I create a function to turn all the chapters into a vector)
getdf<-function(df){
  n<-nrow(df)
  vector<- rep(0,n)
  subject<- df$subject
  for (i in 1:n) {
   vector[i]<-
      as.numeric(substring(str_split(subject[i], "/")[[1]][2], 9))
  }
  return(vector)
}
# Jane and Reed

ggplot() + geom_bar(mapping = aes(getdf(qr_df_2)), stat = "count")+
  labs(x="Section", title = "Frequency of 'Jane and Reed'")


```

## Show virsually

```{r}
word_data<-data.frame(ch = getdf(qr_df_2),num = qr_df_2$numeric.value)

word_data<-word_data %>%
  group_by(ch) %>%
  summarise(count = sum(num))

ggplot(data = word_data, aes(x = as.factor(ch), y = count, group =1)) +
  geom_line(color = "orange") +
  geom_point() +
  labs(x = "Chapter Number", y = "Number of Words")+
  ggtitle("Word Count in Each Chapter")+
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text =  element_text(size = 7))

jrgraph<-tnum.graphTnumList(qr_df_2$subject)
tnum.plotGraph(jrgraph)
```

