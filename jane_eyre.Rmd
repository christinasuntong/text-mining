---
title: "text mining"
author: "Christina"
date: "11/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(gutenbergr)
library(wordcloud)
library(reshape2)
```

# Download and Tidy

```{r}
## Download the book from gutenberg
jane <- gutenberg_download(1260)
## Annotate a 'linenumber' quantity to keep track of lines in the original format and a 'chapter'(with a regex) to find where all the chapters are
original_books <-jane %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  unnest_tokens(word, text)

original_books
## Restructure into the one-token-per-row format with 'unnest_tokens()' function. Here the tokenizing is for words
tidy_books<-original_books %>%
  unnest_tokens(word, word)

length(unique(tidy_books$chapter))
count(tidy_books, chapter) # In this book, there are 39 chapters in it
## Manipulate with tidy tools

# Remove stop words -- remove stop words(kept in the tidytext dataset 'stop_words') with 'anti_join()'
data("stop_words")

tidy_books<-tidy_books %>%
  anti_join(stop_words) #'stop_words' dataset in the tidytext package contains stop words from three lexicons -- "AFINN", "bing","nrc"
  
# Find the most common words with dplyr's 'count()'
tidy_books %>%
  count(word, sort = TRUE) # The word counts are stored in a tidy data frame, allowing us to pipe this directly to the ggplot2 package

# Visualization of the most common words
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) # Here I filter the number of words > 200 in this book and we can see the most common word is jane.
```

# Sentiment analysis with tidy data

In the previous part, I explored what changed the original book into the tidy text format and showed how this format can be used to approach questions about word frequency. Next I would like to address the topic of sentiment analysis. When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative. So here I use the tools of text mining to appraoch the emotional content of text.

```{r}
## Sentiment analysis with inner join
# Look at the words with a joy score from the NRC lexicon
nrc_joy<-get_sentiments("nrc") %>%
  filter(sentiment == "joy") # Choose the name 'word' for the output column from 'unnest_tokens()' and filter for the joy words

# What are the most common joy words in Jane Eyre
tidy_books %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) 

```

I found that mostly positive, happy words about love, hope and happy here.

In addition, I want to examine how sentiment changes throughout each novel. First, I find a sentiment score for each word using the Bing lexicon and 'inner_join()'. Next I count up how many positive and negative words there are in defined sections of the book and define an "index" here to keep track of where we are in the narrative; this index (using integer division) counts up sections of 60 lines of text. In addition, I use 'pivot_wider()' so that we can have negative and positive sentiment in separate columns and finally calculate a net sentiment (positive - negative).

```{r}
# Calculate negative and positive sentiment
jane_sentiment<-tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 60, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

# Plot sentiment scores
ggplot(jane_sentiment,aes(index, sentiment,)) +
  geom_col(show.legend = FALSE) 
  # Notice that here I'm plotting against the 'index' on the x-axis that keeps track of narrative time in sections of text.
```

We can see how the plot of each novel changes toward more positive or negative sentiment over the trajectory of the story. From the plot above, I find that there are more negative sentiments during the early index and more positive sentiments during the late index. I think this fits the story. During Eyre's childhood, she was abuted by her aunt,Sarah Reed, she did not have a happy childhood in her early years. Therefore the sentiment is negative. But when she met Rochester as written in the late plots, she was married with him and had a sweet life. So there are more positive ones here.

# Compare the three sentiment dictionaries

```{r}
afinn<-tidy_books %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber %/% 60) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc<-bind_rows(
  tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing at al."),
  tidy_books %>%
    inner_join(get_sentiments("nrc") %>%
                 filter(sentiment %in% c("positive","negative"))
               ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 60, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative)

#visualize
bind_rows(afinn,
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

#Why the result for the NRC lexicon biased so high in sentiment compared to the Bing et al. result?
get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  count(sentiment)

get_sentiments("bing") %>%
  count(sentiment)
```
#Most common positive and negative words

```{r}
bing_word_counts<-tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

#visualization
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

#Wordclouds

```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

#in other functions
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

#Task three
```{r}
library(devtools)
devtools::install_github("Truenumbers/tnum/tnum")

#'tnum' package
library(knitr)
library(gutenbergr)
library(tidyverse)
library(tnum)

tnum.authorize("mssp1.bu.edu")
tnum.setSpace("test2")

#jane_eyre<-gutenberg_download(gutenberg_id = 1260)  ## download Jane Eyre
jane_eyre_txt<-readLines("jane.txt")
source("Book2TN-v3-hw.R")

#tnBooksFromLines(jane_eyre_txt, "je")


## not run: tnBooksFromLines(jane_eyre$text, "jane_eyre")

tnum.getDBPathList(taxonomy = "subject", levels = 2)
```


```{r echo=TRUE, warning=FALSE, message=FALSE}
q111 <- tnum.query(query = "jane_eyre# has ordinal", max=500)   ## everything
df111 <- tnum.objectsToDf(q111)

## show ordered objects in document
q112 <- tnum.query("jane_eyre# has ordinal")   ## show ordered objects in document
df112 <- tnum.objectsToDf(q112)

## focus on one paragraph -- note the word count for each sentence
q3 <- tnum.query("jane_eyre/section:0001/paragraph:0005# has count#")  # just 1 para
df3 <- tnum.objectsToDf(q3) %>% filter(date == "2021-12-03")
df3


## and now look at the text in a sentence
q1 <- tnum.query("jane_eyre/section:0001/paragraph:0005/sentence:0002# has text")
df1 <- tnum.objectsToDf(q1) %>% filter(date == "2021-12-03")
df1

## To extract a paragraph of text
q4 <- tnum.query("jane_eyre/section:0001/paragraph:0005/sentence# has text", max = 10)
df4 <- tnum.objectsToDf(q4) %>% filter(date == "2021-12-03")
para_text4 <- df4 %>% pull(string.value) %>% 
                      str_replace_all("\"","") %>% 
                      str_flatten(collapse = " ")

## steps to understand
# a <- para_text4[1]
# a
# 
# b <- str_replace_all(a,"\"","")   
# b
# 
# c <- para_text4
# c
# 
# c <- str_replace_all(a,"\"","")  
# 
```

# Use tnum in text analysis

```{r}
##  The ordinal numbers for the entire book 
##  show the sequence of objects in order of their appearance.
w10 <- tnum.query("jane_eyre# has ordinal", max=1800)
wdf10 <- tnum.objectsToDf(w10)

## Examing the first 50 TNs  makes it easy to see the Table of Contents
## and to see that object 22 is the heading at the start of Chapter 1


## This shows the Table of Contents
w11 <- tnum.query("jane_eyre# has text", start = 3 ,max=18)
wdf11 <- tnum.objectsToDf((w11))

table_of_contents <- wdf11 %>% select(string.value) 


## Look at just the headings shows the structure of the book
w13 <- tnum.query("jane_eyre/heading# has text", max=40)
wdf13 <- tnum.objectsToDf(w13)


## It may look like the table of contents is repeated twice,
## but examing the ordinals produces chapter list that includes the 
## ordinal location for the heading of each chapter
w14 <- tnum.query("jane_eyre/heading# has ordinal", max=40)
wdf14 <- tnum.objectsToDf(w14)

chapter_locations <- left_join(select(wdf13, subject, string.value), 
                               select(wdf14, subject, numeric.value)) 
## add column for chapter number
library(magrittr)
chapter_locations %<>% mutate(chapter=1:36)

w15 <- tnum.query("jane_eyre/section:0011# has ordinal")
wdf15 <- tnum.objectsToDf(w15)



#a <- chapter_locations %>% filter(chapter==2) %>% 
                           select(numeric.value) %>% 
                           unlist()

#a <- str_pad(as.character(a),4,side="left",pad="0")

#b <- paste0("wells12/hw12/section:",a,"#", " has ordinal")

#b


## chapter 1 para 1, word counts for the 3 sentences in para 1
q20 <- tnum.query("jane_eyre# has *", max=3)
df20 <- tnum.objectsToDf(q20)

#  chapter locations  ordinal numbers
ord_ch1 <- unlist(tnum.query("jane_eyre/heading:0011# has ordinal"))
ord_ch2 <- unlist(tnum.query("jane_eyre/heading:0012# has ordinal"))


ch1_txt <- tnum.query("jane_eyre/section:0011/paragraph:0002/# has text", max=30)

ch1_txt_df <- tnum.objectsToDf(ch1_txt)
ch1_txt_df$string.value

ch2_txt <- tnum.query("jane_eyre/section:0011/paragraph:0002/sentence:# has *", max=30)
ch2_txt_df <- tnum.objectsToDf(ch2_txt)

ch2_txt_df$string.value

length(ch2_txt_df$string.value)


q21 <- tnum.query("jane_eyre/section:0011/paragraph:0001/# has *", max = 30)
df21 <- tnum.objectsToDf(q21)


####Before
#Explore the TNs that contain Jane Eyre text
## use query to check TNs
q20<-tnum.query("jane_eyre# has *",max = 3)
df20<-tnum.objectsToDf(q20)

q24<-tnum.query("jane_eyre/heading# has *",max = 60)
df24<-tnum.objectsToDf(q24)

q11<-tnum.query("jane_eyre/heading:0011# has *")
df11<-tnum.objectsToDf(q11)
ord_ch1<-unlist(tnum.query("jane_eyre/heading:0011# has ordinal"))

ord_ch2<-unlist(tnum.query("jane_eyre/heading:0012# has ordinal"))

q25<-tnum.query("jane_eyre/heading:0012# has *")
df25<-tnum.objectsToDf(q25)

ch1_txt<-tnum.query("jane_eyre/section:0011/paragraph:0002/# has text", max = 30)
ch1_txt_df<-tnum.objectsToDf(ch1_txt)
ch1_txt_df$string.value

ch2_txt<-tnum.query("jane_eyre/section:0011/paragraph:0002/sentence:# has *", max = 30)
ch2_txt_df<-tnum.objectsToDf(ch2_txt)
ch2_txt_df$string.value

length(ch2_txt_df$string.value)

q11<-tnum.query("jane_eyre/section:0011/paragraph:0001/# has *", max = 30)
df11<-tnum.objectsToDf(q11)
```

#Sentimentr

```{r}
library(sentimentr)
jane_1 <- get_sentences(para_text4)

## to get sentiment scores by sentence
sentiment(jane_1)

## to get sentiment scores aggregated by paragraph
sentiment_by(jane_1)

emotion(jane_1)
profanity(jane_1)

jan<-tidy_books
jan %>%
  get_sentences() %>%
  sentiment() -> jan_senti

jan_senti %>%
  ggplot() + geom_density(aes(sentiment))

jan_senti %>%
  ggplot() + geom_boxplot(aes(x = linenumber, y = sentiment))

jan_senti %>%
  mutate(polarity_level = ifelse(sentiment > 0, "Positive", "Negative")) %>%
  count(linenumber, polarity_level) %>%
  ggplot() + geom_col(aes(y = linenumber, x = n, fill = polarity_level)) +
  theme_minimal()

jane_1 %>%
  get_sentences() %>%
  sentiment_by() %>%
  highlight()

```

#Sense and Sensibility

```{r}
#create tags
tnum.tagByQuery("jane_eyre# has*= REGEXP(\"love\")",adds = ("ref:love"))

tnum.tagByQuery("jane_eyre# has*=REGEXP(\"Jane|Rochester\")",adds = ("ref:JR"))

#make plots(before making plots, I create a function to turn all the chapters into a vector)
getsection<-function(query_t2){
  n= length(query_t2)
  sectionlist<-1:n
  for (i in 1:n) {
    sectionlist[i]<-
      as.character(substring(str_split(tnum.getAttrFromList(query_t2[i],"subject"), "/"), 4))
  }
  return(sectionlist)
}

#love
love<-tnum.query("@ref:love", max = 20)
love_df<-tnum.objectsToDf(love)

ggplot() + geom_bar(mapping = aes(getsection(love)), stat = "count")+
  labs(x="Section", title = "Frequency of 'love'")

#JR
jr<-tnum.query("@ref:JR", max = 50)
jr_df<-tnum.objectsToDf(jr)

ggplot() + geom_bar(mapping = aes(getsection(jr)), stat = "count") +
  labs(x="Section", title = "Frequency of Jane and Rochester")


```

## Couples with love

```{r}
lovejr<-tnum.query("@[ref:love,ref:JR]", max = 100)
jrgraph<-tnum.graphTnumList(jr_df$subject)
tnum.plotGraph(jrgraph)
```



## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
